{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#\n",
    "# If you do not have WordCloud and want to use it, please run this cell to install it. \n",
    "# \n",
    "####################################################\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "####################################################\n",
    "# coding: utf-8       \n",
    "# Copyright 2020 IBM All Rights Reserved.   \n",
    "#   \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");   \n",
    "# you may not use this file except in compliance with the License.   \n",
    "# You may obtain a copy of the License at   \n",
    "#   \n",
    "# http://www.apache.org/licenses/LICENSE-2.0   \n",
    "#   \n",
    "# Unless required by applicable law or agreed to in writing, software   \n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,   \n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   \n",
    "# See the License for the specific language governing permissions and   \n",
    "# limitations under the License.\n",
    "####################################################\n",
    "####################################################\n",
    "#\n",
    "# The data used by this notebook has been generated from various sources including content from the\n",
    "# COVID-19 Open Research Dataset (CORD-19)  (https://pages.semanticscholar.org/coronavirus-research)\n",
    "#\n",
    "####################################################\n",
    "import sys\n",
    "import os\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "use_local_data=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(words_for_cloud):\n",
    "######################################\n",
    "# Given a list of space-delimited words, this function will build\n",
    "# and display a word cloud image\n",
    "######################################\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "#                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(words_for_cloud) \n",
    "# plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_demo():\n",
    "######################################\n",
    "# This function will initialize global variables and load the master index. This index is\n",
    "# in CSV format and contains the main items of information that allow a user to drill \n",
    "# down into the actual raw ACD enrichment detail files in order to perform deeper analysis.\n",
    "######################################\n",
    "    \n",
    "    print(\"Running initialize...\")\n",
    "    \n",
    "    global use_local_data\n",
    "    use_local_data=False\n",
    "    \n",
    "    global public_url_path\n",
    "    public_url_path=\"https://whcs-dev-covid19-data.s3.us-east.cloud-object-storage.appdomain.cloud/\"\n",
    "    \n",
    "    csv_path = public_url_path+\"data_index.csv\"\n",
    "\n",
    "    data_file_names_file=urllib.request.urlopen(public_url_path+\"data_file_names.txt\")\n",
    "\n",
    "    global data_file_names\n",
    "    data_file_names_lines=data_file_names_file.readlines()\n",
    "    data_file_names = []\n",
    "    for datafile in data_file_names_lines:\n",
    "        data_file_names.append(str(datafile,\"utf-8\").strip())\n",
    "\n",
    "    global master_index\n",
    "    master_index = pd.read_csv(csv_path,\n",
    "                               usecols=[\"docId\",\"name\",\"preferredName\"],\n",
    "                               dtype={\"docId\":\"str\"}\n",
    "                              )    #,nrows=15800000)\n",
    "    print(\"...initialize complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_local_data():\n",
    "######################################\n",
    "# This function will initialize global variables and load the master index. This index is\n",
    "# in CSV format and contains the main items of information that allow a user to drill \n",
    "# down into the actual raw ACD enrichment detail files in order to perform deeper analysis.\n",
    "######################################\n",
    "\n",
    "    print(\"Running initialize_local_data...\")\n",
    "\n",
    "    global use_local_data\n",
    "    use_local_data=True\n",
    "\n",
    "    global raw_files_path\n",
    "    \n",
    "    # !!!!!!!!!!!!!!!!!!!!!\n",
    "    # TO DO:  \n",
    "    # !!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    # Set path values for the location of the csv file and the raw data files \n",
    "    # Example:\n",
    "    # csv_path = \"/Users/myname/folder1/folder2/xxxxxx.csv\"\n",
    "    # raw_files_path = \"/Users/myname/raw_files_place\"\n",
    "    \n",
    "    csv_path = \"<.......your csv file path here.......>\"\n",
    "    raw_files_path = \"<.......your raw files root directory here.......>\"\n",
    "\n",
    "    global master_index\n",
    "    master_index = pd.read_csv(csv_path,\n",
    "                               usecols=[\"docId\",\"name\",\"preferredName\"],\n",
    "                               dtype={\"docId\":\"str\"}\n",
    "                              )    #,nrows=15800000)\n",
    "    print(\"...initialize_local_data complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_data_file(dirname):\n",
    "######################################\n",
    "# This function will return the first raw data file it finds in the folder structure\n",
    "# of raw json files. A file is needed to support the methods that list the \n",
    "# names of the data elements that a user might want to use to perform analysis.\n",
    "######################################\n",
    "    if use_local_data:\n",
    "        for (pth, dir, fn) in walk(dirname):\n",
    "            for n in fn:\n",
    "                if n.endswith(\".json\"):\n",
    "                    return os.path.join(pth,n)\n",
    "        return \"no_datafile_found\"\n",
    "    else:\n",
    "        return public_url_path+data_file_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_data_types():\n",
    "######################################\n",
    "# This function uses an arbitrary raw data file to obtain\n",
    "# and list out for the user a list of the data types that \n",
    "# are available for exploration.\n",
    "######################################\n",
    "    print(\"=============================\")\n",
    "    print(\" ACD raw data - data types\")\n",
    "    print(\"=============================\")\n",
    "    if use_local_data:\n",
    "        targetJsonFile=get_first_data_file(raw_files_path)\n",
    "    else:\n",
    "        targetJsonFile=get_first_data_file(\"\")\n",
    "    # read in json file as a dataframe        \n",
    "    jdata = pd.read_json(targetJsonFile)\n",
    "    json_dataframe = pd.DataFrame(jdata)\n",
    "    xresult=json_dataframe.get(key=\"result\")\n",
    "    xunstruc=xresult.get(key=\"unstructured\")\n",
    "    xzero=xunstruc[0]\n",
    "    xdata=xzero[\"data\"]\n",
    "    for i in xdata:\n",
    "        print(f'{i:30}',type(xdata[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_data_type_fields(data_type):\n",
    "######################################\n",
    "# Given a data type, this function uses an arbitrary raw data file to obtain\n",
    "# and list out for the user a list of the fields supporting that \n",
    "# data type. These fields can then be used to get at the lowest level\n",
    "# of ACD enrichment data.\n",
    "######################################\n",
    "    print(\"=============================\")\n",
    "    print(\" ACD raw data - \",data_type,\"field names\")\n",
    "    print(\"=============================\")\n",
    "    if use_local_data:\n",
    "        targetJsonFile=get_first_data_file(raw_files_path)\n",
    "    else:\n",
    "        targetJsonFile=get_first_data_file(\"\")\n",
    "    # read in json file as a dataframe        \n",
    "    jdata = pd.read_json(targetJsonFile)\n",
    "    json_dataframe = pd.DataFrame(jdata)\n",
    "    xresult=json_dataframe.get(key=\"result\")\n",
    "    xunstruc=xresult.get(key=\"unstructured\")\n",
    "    xzero=xunstruc[0]\n",
    "    xdata=xzero[\"data\"]\n",
    "    this_data_type=xdata[data_type]\n",
    "    tdtzero=this_data_type[0]\n",
    "    for i in tdtzero:\n",
    "        print(f'{i:30}',type(tdtzero[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_names(topnamedepth):\n",
    "######################################\n",
    "# This function will list, in ranked order, the attribute names\n",
    "# and the associate preferred names of the concept it is associated\n",
    "# with.  The ranking is done by instance counts of the relationships\n",
    "# across all documents processed by this enrichment run.\n",
    "######################################\n",
    "    print(\"\\n\\n=============================\")\n",
    "    print(\"Top attributeValue Names in Ranked Order of Occurrence\")\n",
    "    print(\"=============================\")\n",
    "    name_rank_index=master_index[\"name\"].value_counts()\n",
    "    nr_len = name_rank_index.size\n",
    "    if nr_len < topnamedepth:\n",
    "        topnamedepth = nr_len\n",
    "    name_list=[]\n",
    "    for x in range(0,topnamedepth):\n",
    "        name_list.append(name_rank_index.index[x])\n",
    "    return name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_name_selection(top_name_list):\n",
    "######################################\n",
    "# Function to prompt for and return the value chosen which corresponds to the \n",
    "# name value that the user wants to work with.\n",
    "######################################\n",
    "    list_size=len(top_name_list)\n",
    "    ct=0\n",
    "    print(\"\\n\\n\")\n",
    "    print(ct,\"Exit\")\n",
    "    for x in top_name_list:\n",
    "        ct += 1\n",
    "        print(ct,x)\n",
    "    top_name_index_int = -1\n",
    "    while top_name_index_int < 0 or top_name_index_int > list_size:\n",
    "        top_name_index=input(\"\\nEnter number of desired name: \")\n",
    "        try:\n",
    "            top_name_index_int=int(top_name_index)\n",
    "        except:\n",
    "            top_name_index_int=-1\n",
    "    return top_name_index_int-1              # allow for zero-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_preferred_names(topprefnamedepth,df_top_name):\n",
    "######################################\n",
    "# This function will list, in ranked order, the attribute names\n",
    "# and the associate preferred names of the concept it is associated\n",
    "# with.  The ranking is done by instance counts of the relationships\n",
    "# across all documents processed by this enrichment run.\n",
    "######################################\n",
    "    print(\"\\n\\n=============================\")\n",
    "    print(\"Top attributeValue Preferred Names in Ranked Order of Occurrence\")\n",
    "    print(\"=============================\")\n",
    "    name_rank_index=df_top_name[\"preferredName\"].value_counts()\n",
    "    nr_len = name_rank_index.size\n",
    "    if nr_len < topprefnamedepth:\n",
    "        topprefnamedepth = nr_len\n",
    "    name_list=[]\n",
    "    for x in range(0,topprefnamedepth):\n",
    "        name_list.append(name_rank_index.index[x])\n",
    "    return name_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_preferred_name_selection(top_preferred_name_list):\n",
    "######################################\n",
    "# Function to prompt for and return the value chosen which corresponds to the \n",
    "# preferred name value that the user wants to work with.\n",
    "######################################\n",
    "    list_size=len(top_preferred_name_list)\n",
    "    ct=0\n",
    "    print(\"\\n\\n\")\n",
    "    print(ct,\"Exit\")\n",
    "    for x in top_preferred_name_list:\n",
    "        ct += 1\n",
    "        print(ct,x)\n",
    "    top_preferred_name_index_int = -1\n",
    "    while top_preferred_name_index_int < 0 or top_preferred_name_index_int > list_size:\n",
    "        top_preferred_name_index=input(\"\\nEnter number of desired name: \")\n",
    "        try:\n",
    "            top_preferred_name_index_int=int(top_preferred_name_index)\n",
    "        except:\n",
    "            top_preferred_name_index_int=-1\n",
    "    return top_preferred_name_index_int-1              #allow for zero-based index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_count():\n",
    "    print(master_index[\"docId\"].value_counts().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_local_data():\n",
    "\n",
    "    list_depth=20\n",
    "    \n",
    "    #############################\n",
    "    # load top names from the ACD Enrichment Result CSV\n",
    "    ############################# \n",
    "    top_name_list=get_top_names(list_depth)\n",
    "    top_name_index=get_top_name_selection(top_name_list)\n",
    "    while top_name_index > -1:\n",
    "        df_top_name = master_index.loc[master_index['name']==top_name_list[top_name_index]]\n",
    "    #############################\n",
    "    # load top preferred names from the ACD Enrichment Result CSV\n",
    "    ############################# \n",
    "        top_preferred_name_list=get_top_preferred_names(list_depth,df_top_name)\n",
    "        top_preferred_name_index=get_top_preferred_name_selection(top_preferred_name_list)\n",
    "        while top_preferred_name_index > -1:\n",
    "            df_top_pref_name = df_top_name.loc[df_top_name['preferredName']==top_preferred_name_list[top_preferred_name_index]]\n",
    "    #############################\n",
    "    # get top documents for preferred names\n",
    "    ############################# \n",
    "            docList=df_top_pref_name[\"docId\"].value_counts()\n",
    "            print(\"\\n========================================================\\n\")\n",
    "            print(docList.size,\"documents were found matching your selection of\",top_name_list[top_name_index],\"and\",top_preferred_name_list[top_preferred_name_index])\n",
    "            print(\"\\nHow many documents do you want to include in your analysis?\")\n",
    "            print(\"Note: Documents will be included in descending order of occurrences per document of your selection.\")\n",
    "            print(\"It is recommended that you choose 500 documents or less, unless you want to wait a long time.\")\n",
    "            doc_count=input()\n",
    "            doc_count=int(doc_count)\n",
    "            if doc_count > 5000:\n",
    "                doc_count=5000\n",
    "            if doc_count > docList.size:\n",
    "                doc_count = docList.size\n",
    "            print(\"Will process\",doc_count,\"files.\")\n",
    "            if doc_count == 0:\n",
    "                break\n",
    "            current_doc_count=0\n",
    "            fflist = os.listdir(raw_files_path)\n",
    "            wordlist=\"\"   \n",
    "            found_atleast_one_doc=False\n",
    "            flist=[]\n",
    "            for (pth, dir, fn) in walk(raw_files_path):\n",
    "                for fnn in fn:\n",
    "                    flist.append(os.path.join(pth,fnn))\n",
    "            for doc_id in docList.index:\n",
    "                #sometimes doc_id can be all numbers, so let's make sure it's a string type\n",
    "                doc_id=str(doc_id)\n",
    "                doc_id_str=str(doc_id)+\"_body\"\n",
    "                found_doc=False\n",
    "                for fname in flist:\n",
    "                    if doc_id_str in fname and fname.endswith(\".json\"):\n",
    "                        targetJsonFile = fname\n",
    "                        found_doc=True\n",
    "                        fount_atleast_one_doc=True\n",
    "                        # read in json file as a dataframe        \n",
    "                        jdata = pd.read_json(targetJsonFile)\n",
    "                        json_dataframe = pd.DataFrame(jdata)\n",
    "                        xresult=json_dataframe.get(key=\"result\")\n",
    "                        xunstruc=xresult.get(key=\"unstructured\")\n",
    "                        if type(xunstruc) is not list:\n",
    "                            continue\n",
    "                        xzero=xunstruc[0]\n",
    "                        if \"data\" in xzero:\n",
    "                            xdata=xzero[\"data\"]\n",
    "                            if \"attributeValues\" in xdata:\n",
    "                                xattrv=xdata[\"attributeValues\"]\n",
    "                                for oneattrv in xattrv:\n",
    "                                    if \"coveredText\" in oneattrv:\n",
    "                                        covt=oneattrv[\"coveredText\"]\n",
    "                                        covt=covt.replace(\" \",\"_\")\n",
    "                                        wordlist=wordlist+\" \"+covt\n",
    "                    if found_doc:\n",
    "                        break\n",
    "                if found_doc:\n",
    "                    current_doc_count += 1\n",
    "                if current_doc_count == doc_count:\n",
    "                    break;\n",
    "            if wordlist==\"\":\n",
    "                if found_atleast_one_doc==False:\n",
    "                    wordlist=\"no_matching_documents\"\n",
    "                else:\n",
    "                    wordlist=\"no_words\"\n",
    "            word_cloud(wordlist)\n",
    "            top_preferred_name_index=get_top_preferred_name_selection(top_preferred_name_list)\n",
    "        top_name_index=get_top_name_selection(top_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_demo():\n",
    "\n",
    "    list_depth=20\n",
    "    \n",
    "    #############################\n",
    "    # load top names from the ACD Enrichment Result CSV\n",
    "    ############################# \n",
    "    top_name_list=get_top_names(list_depth)\n",
    "    top_name_index=get_top_name_selection(top_name_list)\n",
    "    while top_name_index > -1:\n",
    "        df_top_name = master_index.loc[master_index['name']==top_name_list[top_name_index]]\n",
    "    #############################\n",
    "    # load top preferred names from the ACD Enrichment Result CSV\n",
    "    ############################# \n",
    "        top_preferred_name_list=get_top_preferred_names(list_depth,df_top_name)\n",
    "        top_preferred_name_index=get_top_preferred_name_selection(top_preferred_name_list)\n",
    "        while top_preferred_name_index > -1:\n",
    "            df_top_pref_name = df_top_name.loc[df_top_name['preferredName']==top_preferred_name_list[top_preferred_name_index]]\n",
    "    #############################\n",
    "    # get top documents for preferred names\n",
    "    ############################# \n",
    "            docList=df_top_pref_name[\"docId\"].value_counts()\n",
    "            print(\"\\n========================================================\\n\")\n",
    "            print(docList.size,\"documents were found matching your selection of\",top_name_list[top_name_index],\"and\",top_preferred_name_list[top_preferred_name_index])\n",
    "            print(\"\\nHow many documents do you want to include in your analysis?\")\n",
    "            print(\"Note: Documents will be included in descending order of occurrences per document of your selection.\")\n",
    "            print(\"It is recommended that you choose 500 documents or less, unless you want to wait a long time.\")\n",
    "            doc_count=input()\n",
    "            doc_count=int(doc_count)\n",
    "            if doc_count > 5000:\n",
    "                doc_count=5000\n",
    "            if doc_count > docList.size:\n",
    "                doc_count = docList.size\n",
    "            print(\"Will process\",doc_count,\"files.\")\n",
    "            if doc_count == 0:\n",
    "                break\n",
    "            current_doc_count=0\n",
    "            \n",
    "            wordlist=\"\"   \n",
    "            found_atleast_one_doc=False\n",
    "            for doc_id in docList.index:\n",
    "                #sometimes doc_id can be all numbers, so let's make sure it's a string type\n",
    "                doc_id=str(doc_id)\n",
    "                doc_id_str=str(doc_id)+\"_body\"\n",
    "                found_doc=False\n",
    "                for fname in data_file_names:\n",
    "                    if doc_id_str in fname and fname.endswith(\".json\"):\n",
    "                        targetJsonFile = public_url_path+fname\n",
    "                        found_doc=True\n",
    "                        fount_atleast_one_doc=True\n",
    "                        # read in json file as a dataframe        \n",
    "                        jdata = pd.read_json(targetJsonFile)\n",
    "                        json_dataframe = pd.DataFrame(jdata)\n",
    "                        xresult=json_dataframe.get(key=\"result\")\n",
    "                        xunstruc=xresult.get(key=\"unstructured\")\n",
    "                        if type(xunstruc) is not list:\n",
    "                            continue\n",
    "                        xzero=xunstruc[0]\n",
    "                        if \"data\" in xzero:\n",
    "                            xdata=xzero[\"data\"]\n",
    "                            if \"attributeValues\" in xdata:\n",
    "                                xattrv=xdata[\"attributeValues\"]\n",
    "                                for oneattrv in xattrv:\n",
    "                                    if \"coveredText\" in oneattrv:\n",
    "                                        covt=oneattrv[\"coveredText\"]\n",
    "                                        covt=covt.replace(\" \",\"_\")\n",
    "                                        wordlist=wordlist+\" \"+covt\n",
    "                    if found_doc:\n",
    "                        break\n",
    "                if found_doc:\n",
    "                    current_doc_count += 1\n",
    "                if current_doc_count == doc_count:\n",
    "                    break;\n",
    "            if wordlist==\"\":\n",
    "                if found_atleast_one_doc==False:\n",
    "                    wordlist=\"no_matching_documents\"\n",
    "                else:\n",
    "                    wordlist=\"no_words\"\n",
    "            word_cloud(wordlist)\n",
    "            top_preferred_name_index=get_top_preferred_name_selection(top_preferred_name_list)\n",
    "        top_name_index=get_top_name_selection(top_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# run this method to perform all initialization\n",
    "#################################\n",
    "# !!!!!!!!!!!!!!!!!!!!!\n",
    "# TO DO:  If you want to run the demo, uncomment the first statement and comment out the second.  \n",
    "#         If you want to run using locally data, comment out the first statement and uncomment the second.\n",
    "# !!!!!!!!!!!!!!!!!!!!!\n",
    "initialize_demo()\n",
    "#initialize_local_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# run this method to return the number of documents defined in the\n",
    "# master index file\n",
    "#################################\n",
    "get_document_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# run this method to list out the data types that are available in the raw files\n",
    "#################################\n",
    "list_data_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# run this method to list the field names and types for a \n",
    "# given data type (that would be listed by the preceding method)\n",
    "#################################\n",
    "list_data_type_fields(\"attributeValues\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# run this method to perform all initialization\n",
    "#################################\n",
    "global use_local_data\n",
    "if use_local_data:\n",
    "    run_local_data()\n",
    "else:\n",
    "    run_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
